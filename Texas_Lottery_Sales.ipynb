{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e90687ea",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this project, I'm exploring the dataset **\"Texas Lottery® Sales by Fiscal Month/Year, Game and Retailer\"**.\n",
    "\n",
    "The project consists of three distinct parts:\n",
    "\n",
    "- API querying with SoQL\n",
    "- Local data processing with Python and Dask\n",
    "- Data visualization with Tableau\n",
    "\n",
    "This Notebook focuses on the second part: processing the local copy of the dataset using Python and Dask.\n",
    "\n",
    "## Objectives\n",
    "\n",
    "Within the scope of this part, I aim to achieve the following goals:\n",
    "\n",
    "- Preprocessing the Data\n",
    "\n",
    "- Analyzing Seasonal Trends:\n",
    "\n",
    "  - Examining seasonal sales trends\n",
    "  - Analyzing ticket prices over time and identifying seasonal variation\n",
    "\n",
    "- Optimization and Export for Tableau:\n",
    "\n",
    "  - Aggregating data to reduce dataset size\n",
    "  - Imputing missing values and ensuring data completeness\n",
    "\n",
    "## The tools I use\n",
    "\n",
    "- **Python**: Primary programming language used for data manipulation and analysis.\n",
    "- **Dask**: Parallel computing library that scales Python code from single machines to large clusters, used for handling large datasets that do not fit into memory.\n",
    "- **Pandas**: Python library for data manipulation and analysis, used for cleaning and transforming the dataset.\n",
    "- **Matplotlib/Seaborn**: Visualization libraries in Python, employed for creating plots and graphs to explore data trends.\n",
    "- **NumPy**: A fundamental package for scientific computing in Python.\n",
    "- **Jupyter Notebook**: Interactive computing environment used for documenting the data analysis process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffcdf5d9",
   "metadata": {},
   "source": [
    "## Data Sources\n",
    "\n",
    "The dataset **\"Texas Lottery® Sales by Fiscal Month/Year, Game and Retailer\"** has been provided by **Texas Lottery Commission**.\n",
    "\n",
    "- [Link to the Dataset on data.texas.gov](https://data.texas.gov/dataset/Texas-Lottery-Sales-by-Fiscal-Month-Year-Game-and-/beka-uwfq/about_data) \n",
    "- Access & Use: This dataset is intended for public access and use.\n",
    "- Copyright and Trademark Notice: [Link to texaslottery.com](https://www.texaslottery.com/export/sites/lottery/Misc/copyright.html) \n",
    "\n",
    "\n",
    "### Disclosure\n",
    "\n",
    "The dataset utilized in this project was sourced from the Texas Open Data Portal, which is publicly available and free to use. It's important to note, however, that **the Texas Lottery Commission holds copyright and trademark protections over various elements associated with their data and brand.** This encompasses all logos, text, content, including underlying HTML code, designs, and graphics depicted on their Internet website, safeguarded under United States and international copyright and trademark laws and treaties.\n",
    "\n",
    "I do not claim any rights over these elements, and no such material has been reproduced within this project. The use of the dataset is for analytical and educational purposes only, adhering to the guidelines stipulated by the Texas Open Data Portal and respecting the copyright and trademark notice issued by the Texas Lottery Commission. Any specific trademarks or service marks mentioned within the dataset are duly recognized as the property of the Texas Lottery Commission, and their use in this project does not imply any affiliation with or endorsement by the Commission.\n",
    "\n",
    "### Dataset Version\n",
    "\n",
    "Due to server response limitations and to ensure uninterrupted data analysis, a **local copy of the dataset** was downloaded and used for data manipulation and analysis within Python. This approach was adopted to mitigate potential server timeouts and connectivity issues encountered during direct API access, allowing for a more stable and efficient data analysis process.\n",
    "\n",
    "- Data Last Updated: January 25, 2024\n",
    "- Data Coverage: \n",
    "  - Start Date: September 2020\n",
    "  - End Date: January 2024\n",
    "\n",
    "Unfortunately, due to the large size of the dataset file (7.5 GB), it cannot be included in the project's repository because of GitHub's file size limitations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8af639",
   "metadata": {},
   "source": [
    "# 1. Setting up the Environment\n",
    "\n",
    "The following command installs the Python libraries required for this project:\n",
    "\n",
    "`pip install dask pandas numpy matplotlib seaborn ipython \"dask[distributed]\" \"dask[dataframe]\"`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2380e64f",
   "metadata": {},
   "source": [
    "# 2. Loading and Preprocessing the Data\n",
    "## 2.1. Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2ebbbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the Dask Client for distributed computing.\n",
    "from dask.distributed import Client\n",
    "\n",
    "# Dask DataFrame module for parallel computing with large datasets.\n",
    "import dask.dataframe as dd\n",
    "\n",
    "# The garbage collection module to manage memory and perform cleanup.\n",
    "import gc\n",
    "\n",
    "# Essential tool to work with tabular data structures in Python.\n",
    "import pandas as pd\n",
    "\n",
    "# For numerical computations.\n",
    "import numpy as np \n",
    "\n",
    "# For plotting graphs.\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# To format the axis tick labels.\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "\n",
    "# To handle date formatting on the x-axis.\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "# For more advanced data visualization.\n",
    "import seaborn as sns\n",
    "\n",
    "# Configuring Jupyter to display plots inline.\n",
    "%matplotlib inline \n",
    "\n",
    "# Setting the option to display all columns.\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Setting the option to display all rows.\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "# Setting display option to avoid scientific notation.\n",
    "pd.set_option('display.float_format', lambda x: '%.0f' % x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8fc87cd",
   "metadata": {},
   "source": [
    "## 2.2. Starting a Local Dask Client\n",
    "\n",
    "The dataset under analysis, containing nearly 30 million rows or 7.5 GB in size, presents a significant challenge for in-memory processing on a standard personal computer.\n",
    "\n",
    "To address this, I'm going to use **Dask** - an open-source parallel computing library for large-scale data operations. Dask breaks down large datasets into manageable chunks and processes them in parallel, significantly speeding up data computations and analysis.\n",
    "\n",
    "As I initialize a Dask Client, I'll specify a local directory for storing intermediate data. This is useful for handling large datasets or complex computations, as it allows Dask to efficiently manage temporary data and spill over to disk if the memory is insufficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e8ce41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the Dask client with additional configurations.\n",
    "client = Client(\n",
    "    # Directory for intermediate data.\n",
    "    local_directory='C:/PLACEHOLDER/PATH',\n",
    "    memory_limit='4GB',  # Setting a memory limit for each worker.\n",
    "    n_workers=4,  # Number of workers.\n",
    "    processes=True,  # Using processes instead of threads.\n",
    "    threads_per_worker=1  # Number of threads per worker.\n",
    ")\n",
    "\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c763df",
   "metadata": {},
   "source": [
    "### Processes vs. Threads\n",
    "\n",
    "**Processes**: In Python, using processes means that each worker runs in its own independent memory space. This allows for true parallelism because each process is a separate instance of the Python interpreter, and they can run on multiple CPU cores simultaneously.\n",
    "\n",
    "**Threads**: When using threads, multiple threads run within the same process and share the same memory space. However, Python's Global Interpreter Lock (GIL) allows only one thread to execute Python bytecode at a time, which can limit the performance benefits of multithreading for CPU-bound tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba653c2a",
   "metadata": {},
   "source": [
    "## 2.3. Loading the Dataset\n",
    "\n",
    "Dask provides a DataFrame interface that closely mirrors Pandas, allowing users to perform data manipulation and analysis in a familiar way but on larger-than-memory datasets.\n",
    "\n",
    "Initially, when I tried to load the dataset into a Dask DataFrame, I encountered a ValueError:\n",
    "\n",
    "```python\n",
    "dask_dataframe = dd.read_csv('Texas_Lottery_Sales_by_Fiscal_Month_Year_Game_and_Retailer.csv')\n",
    "```\n",
    "\n",
    "Output:\n",
    "\n",
    "```shell\n",
    "ValueError: Mismatched dtypes found in `pd.read_csv`/`pd.read_table`.\n",
    "\n",
    "+-----------------------------+---------+----------+\n",
    "| Column                      | Found   | Expected |\n",
    "+-----------------------------+---------+----------+\n",
    "| Retailer Location Address 2 | object  | float64  |\n",
    "| Scratch Game Number         | float64 | int64    |\n",
    "| Ticket Price                | float64 | int64    |\n",
    "+-----------------------------+---------+----------+\n",
    "\n",
    "The following columns also raised exceptions on conversion:\n",
    "\n",
    "- Retailer Location Address 2\n",
    "  ValueError(\"could not convert string to float: 'SUITE 180'\")\n",
    "\n",
    "Usually this is due to dask's dtype inference failing, and\n",
    "*may* be fixed by specifying dtypes manually by adding:\n",
    "\n",
    "dtype={'Retailer Location Address 2': 'object',\n",
    "       'Scratch Game Number': 'float64',\n",
    "       'Ticket Price': 'float64'}\n",
    "\n",
    "to the call to `read_csv`/`read_table`.\n",
    "\n",
    "```\n",
    "\n",
    "I'm going to follow the recommendation and specify dtypes in my `read_csv()` call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76967f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the data types for the columns causing issues.\n",
    "dtype = {\n",
    "    'Retailer Location Address 2': 'object',\n",
    "    'Scratch Game Number': 'float64',\n",
    "    'Ticket Price': 'float64'\n",
    "}\n",
    "\n",
    "# Loading the dataset into a Dask DataFrame.\n",
    "dask_dataframe = dd.read_csv(\n",
    "    'Texas_Lottery_Sales_by_Fiscal_Month_Year_Game_and_Retailer.csv',\n",
    "    dtype=dtype\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902abb3b",
   "metadata": {},
   "source": [
    "If data is not evenly partitioned, some operations might load too much data into memory at once.\n",
    "\n",
    "I repartition the data to have more, smaller partitions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5ce867",
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_dataframe = dask_dataframe.repartition(\n",
    "    npartitions=dask_dataframe.npartitions * 2\n",
    ")\n",
    "\n",
    "# Persisting the DataFrame after repartitioning.\n",
    "dask_dataframe = dask_dataframe.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f21118",
   "metadata": {},
   "source": [
    "`persist()` computes the data in the DataFrame and stores it in memory. Unlike lazy evaluation (the default behavior in Dask), where computations are deferred until explicitly triggered by an action like `compute()`, persisting the DataFrame ensures that the data is actively held in memory.\n",
    "\n",
    "The benefits of persisting include improved performance, avoiding recomputations, and added stability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d666975a",
   "metadata": {},
   "source": [
    "## 2.4. Initial Inspection\n",
    "\n",
    "The dataset columns' description, provided by **data.texas.gov**:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12dd2356",
   "metadata": {},
   "source": [
    "\n",
    "| Column Name                                                                                                                       | Description                                                                                                                                     | Type        |\n",
    "| --------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------- | ----------- |\n",
    "| Row ID                                                                                                                            | Unique key.                                                                                                                                     | Plain Text  |\n",
    "| Fiscal Year                                                                                                                       | The fiscal year (Sept-Aug, i.e. Sept 2021-Aug 2022 = 2022, Sept 2022-Aug 2023= 2023 etc.) the pack settled/tickets were sold.                   | Number      |\n",
    "| Fiscal Month                                                                                                                      | The fiscal month number (Sept-Aug, Sept =1, Oct=2, etc.) the pack settled/tickets were sold.                                                    | Number      |\n",
    "| Fiscal Month Name and Number                                                                                                      | The fiscal month number and name (Sept - Aug, Sept =1, Oct=2, etc.) the pack settled/tickets were sold.                                         | Plain Text  |\n",
    "| Calendar Year                                                                                                                     | The calendar year the pack settled/tickets were sold.                                                                                           | Number      |\n",
    "| Calendar Month                                                                                                                    | The calendar month number the pack settled/tickets were sold.                                                                                   | Number      |\n",
    "| Calendar Month Name and Number                                                                                                    | The calendar month number and name the pack settled/tickets were sold.                                                                          | Plain Text  |\n",
    "| Month Ending Date                                                                                                                 | The month end date the pack settled/tickets were sold.                                                                                          | Date & Time |\n",
    "| Game Category                                                                                                                     | The type of lottery game; i.e. Scratch, Lotto Texas®, Powerball®, etc.                                                                          | Plain Text  |\n",
    "| Scratch Game Number                                                                                                               | The game number of the scratch ticket.                                                                                                          | Number      |\n",
    "| Ticket Price                                                                                                                      | The price per ticket.                                                                                                                           | Number      |\n",
    "| Retailer License Number                                                                                                           | The retailer license number that sold the ticket.                                                                                               | Number      |\n",
    "| Retailer Location Name                                                                                                            | The retailer location name that sold the ticket.                                                                                                | Plain Text  |\n",
    "| Retailer Number and Location Name                                                                                                 | The retailer location number/location name that sold the ticket. This number is the store number assigned to the location by the owning entity. | Plain Text  |\n",
    "| Retailer Location Address 1                                                                                                       | The address line 1 of the retailer location that sold the ticket.                                                                               | Plain Text  |\n",
    "| Retailer Location Address 2                                                                                                       | The address line 2 of the retailer location that sold the ticket.                                                                               | Plain Text  |\n",
    "| Retailer Location City                                                                                                            | The city of the retailer location that sold the ticket.                                                                                         | Plain Text  |\n",
    "| Retailer Location State                                                                                                           | The state of the retailer location that sold the ticket.                                                                                        | Plain Text  |\n",
    "| Retailer Location Zip Code                                                                                                        | The zip code of the retailer location that sold the ticket.                                                                                     | Plain Text  |\n",
    "| Retailer Location Zip Code +4                                                                                                     | The zip code +4 of the retailer location that sold the ticket.                                                                                  | Plain Text  |\n",
    "| Retailer Location County                                                                                                          | The county of the retailer location that sold the ticket.                                                                                       | Plain Text  |\n",
    "| Owning Entity Retailer Number                                                                                                     | This is the retailer number of the retailer owning entity who is financially responsible for the location where the pack settled/tickets sold.  | Number      |\n",
    "| Owning Entity Retailer Name                                                                                                       | This is the name of the retailer owning entity who is financially responsible for the location where pack settled/tickets sold.                 | Plain Text  |\n",
    "| Owning Entity/Chain Head Number and Name                                                                                          | This is the name and retailer number of the owning entity of the location financially responsible for the pack settled/tickets sold.            | Plain Text  |\n",
    "| Gross Ticket Sales Amount                                                                                                         | This is the gross sales amount of the pack settled/tickets sold.                                                                                | Number      |\n",
    "| Promotional Tickets Amount                                                                                                        | This is the dollar amount of free tickets given away as part of a promotion approved by the Lottery.                                            | Number      |\n",
    "| Cancelled Tickets Amount                                                                                                          | This is the dollar amount of tickets that were printed then cancelled by retailer due to some sort of issue; e.g. printer jam, etc.             | Number      |\n",
    "| Ticket Adjustments Amount                                                                                                         | This is the dollar amount of ticket adjustments made to the retailer's account; e.g. retailer request for adjustment for damaged tickets, etc.  | Number      |\n",
    "| Ticket Returns Amount                                                                                                             | This is the dollar amount in ticket returns processed at the lottery warehouse and adjusted to retailer's account.                              | Number      |\n",
    "| Net Ticket Sales Amount                                                                                                           | This is the net sales amount of the pack settled/tickets sold minus any promotional, cancelled, adjusted or returned tickets.                   | Number      |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5db1a3",
   "metadata": {},
   "source": [
    "### Dataset Shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f59d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of columns.\n",
    "num_columns = len(dask_dataframe.columns)\n",
    "print(f\"Number of Columns: {num_columns}\")\n",
    "\n",
    "# Number of rows.\n",
    "num_rows = dask_dataframe.shape[0].compute()\n",
    "print(f\"Number of Rows: {num_rows}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f454ade6",
   "metadata": {},
   "source": [
    "Our dataset has 30 columns and almost 30 million rows (entries). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e255f5",
   "metadata": {},
   "source": [
    "### Viewing the First Few Rows\n",
    "\n",
    "I examine the first few rows of the dataset to get a sense of the data structure and contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d68dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d13ed0",
   "metadata": {},
   "source": [
    "## 2.5. Pruning Irrelevant Features\n",
    "\n",
    "For large datasets, especially those that are close to or exceed the system's memory capacity, it can be beneficial to remove unnecessary columns early. This can reduce memory usage and improve processing speed.\n",
    "\n",
    "Columns that I consider redundant or irrelevant to this project's analysis:\n",
    "\n",
    "- `Fiscal Month Name and Number`\n",
    "- `Calendar Month Name and Number`\n",
    "- `Month Ending Date`\n",
    "- `Scratch Game Number`\n",
    "- `Retailer License Number`\n",
    "- `Retailer Number and Location Name`\n",
    "- `Retailer Location Address 1`\n",
    "- `Retailer Location Address 2`\n",
    "- `Retailer Location State`\n",
    "- `Retailer Location Zip Code`\n",
    "- `Retailer Location Zip Code +4`\n",
    "- `Owning Entity Retailer Number`\n",
    "- `Owning Entity Retailer Name`\n",
    "- `Owning Entity/Chain Head Number and Name`\n",
    "- `Ticket Adjustments Amount`\n",
    "\n",
    "Regarding the `Retailer Location State` column, the assumption is that the entire dataset belongs to Texas. However, this assumption requires verification before the column can be safely removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53aea99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counting the number of unique values in the \"Retailer Location State\" column.\n",
    "unique_states = dask_dataframe['Retailer Location State'].nunique().compute()\n",
    "unique_states"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081d2346",
   "metadata": {},
   "source": [
    "There are two unique values in the `Retailer Location State` column.\n",
    "\n",
    "Let's check what they are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee26994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the unique values.\n",
    "unique_states = dask_dataframe['Retailer Location State'].unique().compute()\n",
    "\n",
    "# Converting to a list for easier viewing.\n",
    "unique_states_list = unique_states.tolist()\n",
    "print(\"Unique values in the 'Retailer Location State' column: \"\n",
    "      f\"{unique_states_list}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e490770",
   "metadata": {},
   "source": [
    "The other state is Tennessee.\n",
    "\n",
    "Checking how many rows are attributed to it in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe01a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_counts = dask_dataframe['Retailer Location State'].value_counts().compute()\n",
    "state_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9c50e6",
   "metadata": {},
   "source": [
    "There is one row corresponding to the State of Tennessee. Given that all other rows are related to Texas, this lone Tennessee row can be considered an outlier and removed. Consequently, the `Retailer Location State` column becomes redundant and can also be safely removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0993b7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering the DataFrame to exclude rows where \"Retailer Location State\" is \"TN\".\n",
    "filtered_dask_dataframe = dask_dataframe[\n",
    "    dask_dataframe['Retailer Location State'] != 'TN'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56031be",
   "metadata": {},
   "source": [
    "Dropping the rest of the columns I listed for removal earlier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130fd8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_remove = [\n",
    "    'Fiscal Year',\n",
    "    'Fiscal Month',\n",
    "    'Fiscal Month Name and Number',\n",
    "    'Calendar Month Name and Number',\n",
    "    'Month Ending Date',\n",
    "    'Scratch Game Number',\n",
    "    'Retailer License Number',\n",
    "    'Retailer Location Name',\n",
    "    'Retailer Number and Location Name',\n",
    "    'Retailer Location Address 1',\n",
    "    'Retailer Location Address 2',\n",
    "    'Retailer Location State',\n",
    "    'Retailer Location Zip Code',\n",
    "    'Retailer Location Zip Code +4',\n",
    "    'Owning Entity Retailer Number',\n",
    "    'Owning Entity Retailer Name',\n",
    "    'Owning Entity/Chain Head Number and Name',\n",
    "    'Ticket Adjustments Amount'\n",
    "]\n",
    "\n",
    "# Dropping the specified columns from the DataFrame.\n",
    "reduced_dask_dataframe = filtered_dask_dataframe.drop(columns=columns_to_remove)\n",
    "\n",
    "# Persisting the reduced DataFrame to compute the operation and optimize\n",
    "# further computations.\n",
    "reduced_dask_dataframe = reduced_dask_dataframe.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e30ca63",
   "metadata": {},
   "source": [
    "Checking the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c66978",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_dask_dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de02bd8c",
   "metadata": {},
   "source": [
    "## 2.6. Data Types Check\n",
    "\n",
    "Ensuring that each column is of the correct data type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42200b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_dask_dataframe.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7cac49e",
   "metadata": {},
   "source": [
    "Data types validated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b05026",
   "metadata": {},
   "source": [
    "## 2.7. Summary Statistics\n",
    "\n",
    "Generating summary statistics for numerical columns to understand their distribution, identify any obvious outliers, or spot missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5388452f",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_dask_dataframe.describe().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1f3326",
   "metadata": {},
   "source": [
    "From these summary statistics, the following columns appear right-skewed, indicating a concentration of lower values with fewer higher value outliers:\n",
    "\n",
    "- `Ticket Price`\n",
    "- `Gross Ticket Sales Amount`\n",
    "- `Net Ticket Sales Amount`\n",
    "\n",
    "And the following columns appear left-skewed, with a majority of values clustering towards the higher end and some extreme lower value outliers:\n",
    "\n",
    "- `Promotional Tickets Amount`\n",
    "- `Cancelled Tickets Amount`\n",
    "- `Ticket Returns Amount`\n",
    "\n",
    "However, it's important to note that the presence of extreme outliers in these columns, potentially with the exception of `Ticket Price`, might significantly influence these assessments. Such outliers can distort the mean and give an exaggerated sense of skewness. Therefore, these summary statistics alone might not fully capture the distribution patterns of the listed columns. Visualizing the data distributions could provide a more nuanced understanding of their characteristics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647b6e8b",
   "metadata": {},
   "source": [
    "## 2.8. Missing Values\n",
    "\n",
    "### Identifying Missing Values\n",
    "\n",
    "Checking for missing values in each column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c87934",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_dask_dataframe.isnull().sum().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44502f47",
   "metadata": {},
   "source": [
    "As we can see, there is a large number of missing values in the column `Ticket Price`.\n",
    "\n",
    "To understand this issue better, I'm going to analyze the distribution of null values within the `Game Category` column. This investigation will help identify if missing values are concentrated in specific categories and guide my approach to addressing these gaps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3302eb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping by \"Game Category\" and counting missing values in \"Ticket Price\"\n",
    "# for each category.\n",
    "missing_values_distribution = (\n",
    "    reduced_dask_dataframe\n",
    "    .groupby('Game Category')['Ticket Price']\n",
    "    .apply(lambda x: x.isna().sum(), meta=('x', 'int64'))\n",
    "    .compute()\n",
    ")\n",
    "\n",
    "# Displaying the result.\n",
    "missing_values_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17cde401",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping by \"Game Category\" and counting non-missing values in \"Ticket Price\"\n",
    "# for each category.\n",
    "nonnull_values_distribution = (\n",
    "    reduced_dask_dataframe\n",
    "    .groupby('Game Category')['Ticket Price']\n",
    "    .apply(lambda x: x.notna().sum(), meta=('x', 'int64'))\n",
    "    .compute()\n",
    ")\n",
    "\n",
    "# Displaying the result.\n",
    "nonnull_values_distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a3fe95",
   "metadata": {},
   "source": [
    "The missing values in the `Ticket Price` column are because this column is only populated for the `Scratch Tickets` game category."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa5575a",
   "metadata": {},
   "source": [
    "## 2.9. Duplicate Rows\n",
    "\n",
    "### Checking for Duplicates\n",
    "\n",
    "Unlike Pandas, Dask DataFrames do not have a direct equivalent to the `duplicated()` method. My initial attempts to count the duplicated rows resulted in exceeding the memory budget:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f55d39",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "```python\n",
    "deduplicated_df = reduced_dask_dataframe.drop_duplicates().persist()\n",
    "row_count = reduced_dask_dataframe.shape[0].compute()\n",
    "deduplicated_row_count = deduplicated_df.shape[0].compute()\n",
    "number_of_duplicates = row_count - deduplicated_row_count\n",
    "print(f\"Number of Duplicate Rows: {number_of_duplicates}\")\n",
    "```\n",
    "\n",
    "\n",
    "```python\n",
    "duplicates_count = reduced_dask_dataframe.groupby('Row ID').size().compute()\n",
    "duplicates_count = duplicates_count[duplicates_count > 1]\n",
    "print(f\"Number of Duplicate 'Row ID's: {len(duplicates_count)}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68182833",
   "metadata": {},
   "source": [
    "After encountering those limitations, I adopted the Map-Reduce approach. I used `map_partitions` to apply a function to each partition of my DataFrame and then aggregated the results. The idea here is to identify duplicates within each partition first (map step) and then combine these results to identify global duplicates (reduce step).\n",
    "\n",
    "Before doing this, I ran `gc.collect()`, which triggers Python's garbage collection process. This process reclaims memory by clearing unused objects. The number returned represents the count of unreachable objects found and freed during that garbage collection cycle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e7af48",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd538acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function to apply to each partition.\n",
    "def find_duplicates(partition):\n",
    "    # Finding duplicated \"Row ID\" within the partition.\n",
    "    duplicated = partition[partition.duplicated('Row ID')]\n",
    "    return duplicated\n",
    "\n",
    "# Applying the function to each partition and computing to get results.\n",
    "duplicates_per_partition = (\n",
    "    reduced_dask_dataframe\n",
    "    .map_partitions(find_duplicates)\n",
    "    .compute()\n",
    ")\n",
    "\n",
    "# Now \"duplicates_per_partition\" contains all duplicates found in each chunk.\n",
    "duplicates_per_partition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f60d77",
   "metadata": {},
   "source": [
    "The output is an empty table, meaning there were no duplicates found in the partitions. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0cfaf5",
   "metadata": {},
   "source": [
    "## 2.10. Inspecting Categories\n",
    "\n",
    "Inspecting unique values in the `Game Category` column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a289bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_game_categories = (\n",
    "    reduced_dask_dataframe['Game Category'].unique().compute()\n",
    ")\n",
    "unique_game_categories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54f65cb",
   "metadata": {},
   "source": [
    "There is no inconsistency.\n",
    "\n",
    "Checking the number of unique values in the `Retailer Location City` column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c745bb09",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_cities = (\n",
    "    reduced_dask_dataframe['Retailer Location City'].nunique().compute()\n",
    ")\n",
    "print(f\"Number of unique Retailer Location Cities: {unique_cities}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c14da1",
   "metadata": {},
   "source": [
    "The column contains 1 242 unique values. While inspecting and standardizing these values manually is impractical due to their volume, converting all city names to a uniform text case (e.g., all lowercase or all uppercase) could  create issues in Tableau downstream. Given these potential complications and the fact that I won't be using the column in my Python analysis, I have decided to leave the `Retailer Location City` column unchanged and handle any inconsistencies directly in Tableau.\n",
    "\n",
    "Let's take a look at the `Retailer Location County` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67da35d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the number of unique values in \"Retailer Location County\" column.\n",
    "unique_counties = (\n",
    "    reduced_dask_dataframe['Retailer Location County'].nunique().compute()\n",
    ")\n",
    "unique_counties"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cde1e6d",
   "metadata": {},
   "source": [
    "Our dataset contains 250 unique counties, whereas the state of Texas actually has 254 counties. Before exporting the aggregated dataset for use in Tableau, I will add the missing counties and impute the missing values. This will ensure a complete and accurate geographical visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33658e9a",
   "metadata": {},
   "source": [
    "# 3. Analyzing Seasonal Trends\n",
    "\n",
    "I'll focus here on the types of temporal analysis that aren't going to be covered by the Tableau dashboard downstream.\n",
    "\n",
    "## 3.1. Seasonal Sales Trends\n",
    "\n",
    "Preparing data for plotting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b515b49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Groupping by \"Calendar Year\" and \"Calendar Month\",\n",
    "# then summing \"Gross Ticket Sales Amount\".\n",
    "aggregated_sales = (\n",
    "    reduced_dask_dataframe\n",
    "    .groupby(['Calendar Year', 'Calendar Month'])\n",
    "    ['Net Ticket Sales Amount']\n",
    "    .sum()\n",
    "    .compute()\n",
    ")\n",
    "\n",
    "# Resetting index to convert the Series to a DataFrame.\n",
    "aggregated_sales = aggregated_sales.reset_index()\n",
    "\n",
    "# Combining \"Calendar Year\" and \"Calendar Month\" into a single datetime column\n",
    "# for easier plotting.\n",
    "aggregated_sales['Date'] = pd.to_datetime(\n",
    "    aggregated_sales['Calendar Year'].astype(str)\n",
    "    + '-'\n",
    "    + aggregated_sales['Calendar Month'].astype(str)\n",
    ")\n",
    "\n",
    "# Sorting the DataFrame by \"Date\" to ensure the line chart follows\n",
    "# a chronological order.\n",
    "aggregated_sales = aggregated_sales.sort_values('Date')\n",
    "\n",
    "# Extracting month and day for seasonal trend plotting.\n",
    "aggregated_sales['Month'] = aggregated_sales['Date'].dt.month\n",
    "aggregated_sales['Year'] = aggregated_sales['Date'].dt.year"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b44dd3",
   "metadata": {},
   "source": [
    "Plotting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6bbec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the seaborn style.\n",
    "sns.set(style=\"whitegrid\", rc={\"axes.facecolor\": \"whitesmoke\"})\n",
    "\n",
    "# Creating a figure.\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Defining the time segments, excluding the incomplete year 2020.\n",
    "years = aggregated_sales['Year'].unique()\n",
    "for year in years:\n",
    "    if year > 2020 and year <= 2023:  # Filtering years within the range.\n",
    "        subset = aggregated_sales[aggregated_sales['Year'] == year]\n",
    "        sns.lineplot(\n",
    "            data=subset,\n",
    "            x='Month',\n",
    "            y='Net Ticket Sales Amount',\n",
    "            label=f'{year}'\n",
    "        )\n",
    "\n",
    "# Formatting the plot.\n",
    "plt.title('Total Net Ticket Sales Amount Over Time (Seasonal Trends)', fontsize=16)\n",
    "plt.xlabel('')\n",
    "plt.ylabel('Net Ticket Sales Amount, $', fontsize=14)\n",
    "plt.xticks(\n",
    "    ticks=range(1, 13),\n",
    "    labels=[\n",
    "        'Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun',\n",
    "        'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'\n",
    "    ],\n",
    "    fontsize=12\n",
    ")\n",
    "plt.yticks(fontsize=12)\n",
    "\n",
    "# Formatting y-axis to avoid scientific notation.\n",
    "plt.gca().yaxis.set_major_formatter(FuncFormatter(lambda x, _: f'{x:,.0f}'))\n",
    "\n",
    "plt.grid(True, color='white')\n",
    "sns.despine(left=True, bottom=True)  # Removing the frame.\n",
    "plt.legend(title='Year', fontsize=12, title_fontsize=14)  # Adding legend.\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd2ead0",
   "metadata": {},
   "source": [
    "Seasonal trends observed from 2021 to 2023 indicate the following patterns: sales drop significantly in February but rise again in March. From March onwards, sales decrease, reaching a local minimum in June. There is an increase in July followed by another decrease. Typically, there is another peak in sales from November to January."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ce3851",
   "metadata": {},
   "source": [
    "## 3.2. Ticket Price Over Time\n",
    "\n",
    "Let's take a look at how `Ticket Price` changed over the years.\n",
    "\n",
    "Preparing data for plotting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00425fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Groupping by \"Calendar Year\" and \"Calendar Month\", then computing\n",
    "# the average \"Ticket Price\".\n",
    "average_price = (\n",
    "    reduced_dask_dataframe\n",
    "    .groupby(['Calendar Year', 'Calendar Month'])\n",
    "    ['Ticket Price']\n",
    "    .mean()\n",
    "    .compute()\n",
    ")\n",
    "\n",
    "# Resetting index to convert the MultiIndex DataFrame to a flat DataFrame.\n",
    "average_price = average_price.reset_index()\n",
    "\n",
    "# Creating a \"Date\" column that combines \"Calendar Year\" and \"Calendar Month\"\n",
    "# for plotting.\n",
    "average_price['Date'] = pd.to_datetime(\n",
    "    average_price['Calendar Year'].astype(str)\n",
    "    + '-'\n",
    "    + average_price['Calendar Month'].astype(str)\n",
    ")\n",
    "\n",
    "# Sorting by date to ensure the line chart follows chronological order.\n",
    "average_price = average_price.sort_values(by='Date')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3284bfdb",
   "metadata": {},
   "source": [
    "Plotting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5825f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the seaborn style.\n",
    "sns.set(style=\"whitegrid\", rc={\"axes.facecolor\": \"whitesmoke\"})\n",
    "\n",
    "# Creating a figure.\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plotting.\n",
    "sns.lineplot(data=average_price, x='Date', y='Ticket Price')\n",
    "\n",
    "# Formatting the plot.\n",
    "plt.title('Average Ticket Price Over Time', fontsize=16)\n",
    "plt.xlabel('Date', fontsize=14)\n",
    "plt.ylabel('Average Ticket Price, $', fontsize=14)\n",
    "\n",
    "# Customizing the x-axis date format.\n",
    "date_format = mdates.DateFormatter(\"%b %Y\")\n",
    "plt.gca().xaxis.set_major_formatter(date_format)\n",
    "\n",
    "plt.xticks(rotation=45, fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.grid(True, color='white')\n",
    "sns.despine(left=True, bottom=True)  # Removing the frame.\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b90dfd",
   "metadata": {},
   "source": [
    "## 3.3. Ticket Price Seasonal Trends\n",
    "\n",
    "Preparing data for plotting. I'll use the already existing `average_price` DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76b3e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting month and day for seasonal trend plotting.\n",
    "average_price['Month'] = average_price['Date'].dt.month\n",
    "average_price['Year'] = average_price['Date'].dt.year"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba2e6c6",
   "metadata": {},
   "source": [
    "Plotting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf303d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the seaborn style.\n",
    "sns.set(style=\"whitegrid\", rc={\"axes.facecolor\": \"whitesmoke\"})\n",
    "\n",
    "# Creating a figure.\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Defining the time segments, excluding the incomplete year 2020.\n",
    "years = average_price['Year'].unique()\n",
    "# Sorting years to ensure the legend matches the plot order.\n",
    "for year in sorted(years, reverse=True):\n",
    "    if year > 2020 and year <= 2023:  # Filtering years within the range.\n",
    "        subset = average_price[average_price['Year'] == year]\n",
    "        sns.lineplot(data=subset, x='Month', y='Ticket Price', label=f'{year}')\n",
    "\n",
    "# Formatting the plot.\n",
    "plt.title('Average Ticket Price Over Time (Seasonal Trends)', fontsize=16)\n",
    "plt.xlabel('')\n",
    "plt.ylabel('Average Ticket Price, $', fontsize=14)\n",
    "plt.xticks(\n",
    "    ticks=range(1, 13),\n",
    "    labels=[\n",
    "        'Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun',\n",
    "        'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'\n",
    "    ],\n",
    "    fontsize=12\n",
    ")\n",
    "plt.yticks(fontsize=12)\n",
    "plt.grid(True, color='white')\n",
    "sns.despine(left=True, bottom=True)  # Removing the frame.\n",
    "\n",
    "# Customizing legend to match the plot order.\n",
    "handles, labels = plt.gca().get_legend_handles_labels()\n",
    "order = [labels.index('2023'), labels.index('2022'), labels.index('2021')]\n",
    "plt.legend(\n",
    "    [handles[i] for i in order], [labels[i] for i in order],\n",
    "    title='Year',\n",
    "    fontsize=12,\n",
    "    title_fontsize=14\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a6081a",
   "metadata": {},
   "source": [
    "According to our plot for the years 2021-2023, we can observe a rapid increase in ticket prices starting from January and peaking between April and June. From there, the price remains relatively stable or slightly decreases until the next winter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a972b6a",
   "metadata": {},
   "source": [
    "# 4. Optimization and Export for Tableau\n",
    "\n",
    "## 4.1. Aggregation\n",
    "\n",
    "Aggregating the data to reduce the dataset size and make it suitable for use in Tableau:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee74b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing the aggregation.\n",
    "monthly_agg = reduced_dask_dataframe.groupby([\n",
    "    'Retailer Location County',\n",
    "    'Retailer Location City',\n",
    "    'Game Category',\n",
    "    'Calendar Year',\n",
    "    'Calendar Month'\n",
    "]).agg({\n",
    "    'Gross Ticket Sales Amount': 'sum',\n",
    "    'Net Ticket Sales Amount': 'sum',\n",
    "    'Promotional Tickets Amount': 'sum',\n",
    "    'Cancelled Tickets Amount': 'sum',\n",
    "    'Ticket Returns Amount': 'sum',\n",
    "    'Ticket Price': 'sum'\n",
    "})\n",
    "\n",
    "# Computing the result to get a Pandas DataFrame.\n",
    "monthly_df = monthly_agg.compute()\n",
    "\n",
    "# Resetting the index to turn the grouped columns into regular columns.\n",
    "monthly_df_reset = monthly_df.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d2ad6f",
   "metadata": {},
   "source": [
    "Removing the `Ticket Price` column, as it contains data for only one game category and thus won't be used in the Tableau dashboard:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319929f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_df_reset = monthly_df_reset.drop(columns=['Ticket Price'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02222410",
   "metadata": {},
   "source": [
    "## 4.2. Imputing Missing Values\n",
    "\n",
    "As observed earlier, our dataset contains data for only 250 out of the 254 Texas counties. These missing values will become evident on the Tableau dashboard.\n",
    "\n",
    "To address this, I will create a DataFrame with all possible combinations of Counties and Measurement Types. This will allow me to identify and impute the missing values, ensuring a complete and accurate representation on the dashboard.\n",
    "\n",
    "Below is the list of all counties in Texas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6033435",
   "metadata": {},
   "outputs": [],
   "source": [
    "counties_list = [\n",
    "    'Anderson', 'Andrews', 'Angelina', 'Aransas', 'Archer', 'Armstrong', \n",
    "    'Atascosa', 'Austin', 'Bailey', 'Bandera', 'Bastrop', 'Baylor', 'Bee',\n",
    "    'Bell', 'Bexar', 'Blanco', 'Borden', 'Bosque', 'Bowie', 'Brazoria',\n",
    "    'Brazos', 'Brewster', 'Briscoe', 'Brooks', 'Brown', 'Burleson', 'Burnet',\n",
    "    'Caldwell', 'Calhoun', 'Callahan', 'Cameron', 'Camp', 'Carson', 'Cass',\n",
    "    'Castro', 'Chambers', 'Cherokee', 'Childress', 'Clay', 'Cochran', 'Coke',\n",
    "    'Coleman', 'Collin', 'Collingsworth', 'Colorado', 'Comal', 'Comanche',\n",
    "    'Concho', 'Cooke', 'Coryell', 'Cottle', 'Crane', 'Crockett', 'Crosby',\n",
    "    'Culberson', 'Dallam', 'Dallas', 'Dawson', 'Deaf Smith', 'Delta', 'Denton',\n",
    "    'DeWitt', 'Dickens', 'Dimmit', 'Donley', 'Duval', 'Eastland', 'Ector',\n",
    "    'Edwards', 'Ellis', 'El Paso', 'Erath', 'Falls', 'Fannin', 'Fayette', \n",
    "    'Fisher', 'Floyd', 'Foard', 'Fort Bend', 'Franklin', 'Freestone', 'Frio',\n",
    "    'Gaines', 'Galveston', 'Garza', 'Gillespie', 'Glasscock', 'Goliad',\n",
    "    'Gonzales', 'Gray', 'Grayson', 'Gregg', 'Grimes', 'Guadalupe', 'Hale',\n",
    "    'Hall', 'Hamilton', 'Hansford', 'Hardeman', 'Hardin', 'Harris', 'Harrison',\n",
    "    'Hartley', 'Haskell', 'Hays', 'Hemphill', 'Henderson', 'Hidalgo', 'Hill',\n",
    "    'Hockley', 'Hood', 'Hopkins', 'Houston', 'Howard', 'Hudspeth', 'Hunt',\n",
    "    'Hutchinson', 'Irion', 'Jack', 'Jackson', 'Jasper', 'Jeff Davis',\n",
    "    'Jefferson', 'Jim Hogg', 'Jim Wells', 'Johnson', 'Jones', 'Karnes',\n",
    "    'Kaufman', 'Kendall', 'Kenedy', 'Kent', 'Kerr', 'Kimble', 'King', 'Kinney',\n",
    "    'Kleberg', 'Knox', 'Lamar', 'Lamb', 'Lampasas', 'La Salle', 'Lavaca',\n",
    "    'Lee', 'Leon', 'Liberty', 'Limestone', 'Lipscomb', 'Live Oak', 'Llano',\n",
    "    'Loving', 'Lubbock', 'Lynn', 'McCulloch', 'McLennan', 'McMullen',\n",
    "    'Madison', 'Marion', 'Martin', 'Mason', 'Matagorda', 'Maverick', 'Medina',\n",
    "    'Menard', 'Midland', 'Milam', 'Mills', 'Mitchell', 'Montague',\n",
    "    'Montgomery', 'Moore', 'Morris', 'Motley', 'Nacogdoches', 'Navarro',\n",
    "    'Newton', 'Nolan', 'Nueces', 'Ochiltree', 'Oldham', 'Orange', 'Palo Pinto', \n",
    "    'Panola', 'Parker', 'Parmer', 'Pecos', 'Polk', 'Potter', 'Presidio',\n",
    "    'Rains', 'Randall', 'Reagan', 'Real', 'Red River', 'Reeves', 'Refugio',\n",
    "    'Roberts', 'Robertson', 'Rockwall', 'Runnels', 'Rusk', 'Sabine',\n",
    "    'San Augustine', 'San Jacinto', 'San Patricio', 'San Saba', 'Schleicher',\n",
    "    'Scurry', 'Shackelford', 'Shelby', 'Sherman', 'Smith', 'Somervell',\n",
    "    'Starr', 'Stephens', 'Sterling', 'Stonewall', 'Sutton', 'Swisher',\n",
    "    'Tarrant', 'Taylor', 'Terrell', 'Terry', 'Throckmorton', 'Titus',\n",
    "    'Tom Green', 'Travis', 'Trinity', 'Tyler', 'Upshur', 'Upton', 'Uvalde',\n",
    "    'Val Verde', 'Van Zandt', 'Victoria', 'Walker', 'Waller', 'Ward',\n",
    "    'Washington', 'Webb', 'Wharton', 'Wheeler', 'Wichita', 'Wilbarger',\n",
    "    'Willacy', 'Williamson', 'Wilson', 'Winkler', 'Wise', 'Wood', 'Yoakum',\n",
    "    'Young', 'Zapata', 'Zavala'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f120693a",
   "metadata": {},
   "source": [
    "Generating a DataFrame of all possible combinations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9ca260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting unique values from relevant columns.\n",
    "game_categories = monthly_df_reset['Game Category'].unique()\n",
    "years = monthly_df_reset['Calendar Year'].unique()\n",
    "months = monthly_df_reset['Calendar Month'].unique()\n",
    "\n",
    "# Creating a DataFrame from the Cartesian product of the unique values\n",
    "# and the \"counties_list\".\n",
    "all_combinations = pd.MultiIndex.from_product(\n",
    "    [counties_list, game_categories, years, months], \n",
    "    names=[\n",
    "        'Retailer Location County',\n",
    "        'Game Category',\n",
    "        'Calendar Year',\n",
    "        'Calendar Month'\n",
    "    ]\n",
    ").to_frame(index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf723f4",
   "metadata": {},
   "source": [
    "Merging with the original DataFrame to ensure all combinations are present:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac5ccc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(\n",
    "    all_combinations,\n",
    "    monthly_df_reset,\n",
    "    on=[\n",
    "        'Retailer Location County',\n",
    "        'Game Category',\n",
    "        'Calendar Year',\n",
    "        'Calendar Month'\n",
    "    ],\n",
    "    how='outer'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c32f95a",
   "metadata": {},
   "source": [
    "Cleaning out the rows that fall outside of our original time range:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e533ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing rows for 2020 up to and including August.\n",
    "merged_df = merged_df[\n",
    "    ~(\n",
    "        (merged_df['Calendar Year'] == 2020)\n",
    "        & (merged_df['Calendar Month'] <= 8)\n",
    "    )\n",
    "]\n",
    "\n",
    "# Removing rows for 2024 from February onwards.\n",
    "merged_df = merged_df[\n",
    "    ~(\n",
    "        (merged_df['Calendar Year'] == 2024)\n",
    "        & (merged_df['Calendar Month'] >= 2)\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616ade94",
   "metadata": {},
   "source": [
    "Filling missing values for newly created rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e350826e",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.fillna(\n",
    "    {\n",
    "        'Promotional Tickets Amount': 0,\n",
    "        'Cancelled Tickets Amount': 0, \n",
    "        'Ticket Returns Amount': 0\n",
    "    },\n",
    "    inplace=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5a6882",
   "metadata": {},
   "source": [
    "For the \"Gross Ticket Sales Amount\" and \"Net Ticket Sales Amount\" columns, I replace the missing values with the mean value based on the corresponding \"Game Category\", \"Calendar Year\", and \"Calendar Month\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2f8794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating mean values.\n",
    "mean_values = (\n",
    "    monthly_df_reset\n",
    "    .groupby(['Game Category', 'Calendar Year', 'Calendar Month'])\n",
    "    [['Gross Ticket Sales Amount', 'Net Ticket Sales Amount']]\n",
    "    .mean()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Merging the mean values back into the merged DataFrame.\n",
    "merged_df = pd.merge(\n",
    "    merged_df,\n",
    "    mean_values,\n",
    "    on=['Game Category', 'Calendar Year', 'Calendar Month'],\n",
    "    how='left',\n",
    "    suffixes=('', '_mean')\n",
    ")\n",
    "\n",
    "# Replacing missing values with the mean.\n",
    "for column in ['Gross Ticket Sales Amount', 'Net Ticket Sales Amount']:\n",
    "    merged_df[column] = np.where(\n",
    "        merged_df[column].isna(),\n",
    "        merged_df[column + '_mean'],\n",
    "        merged_df[column]\n",
    "    )\n",
    "\n",
    "# Dropping the mean columns as they are no longer needed.\n",
    "merged_df.drop(\n",
    "    ['Gross Ticket Sales Amount_mean', 'Net Ticket Sales Amount_mean'],\n",
    "    axis=1,\n",
    "    inplace=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3173f39",
   "metadata": {},
   "source": [
    "`merged_df` now contains our original data plus any missing combinations, with `Gross Ticket Sales Amount` and `Net Ticket Sales Amount` filled with mean values where necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a01bc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a05c9f",
   "metadata": {},
   "source": [
    "Saving the DataFrame to a CSV file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b77988",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.to_csv('monthly_aggregation_imputed_.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72fa5423",
   "metadata": {},
   "source": [
    "## 4.3. Creating a Pivot Table\n",
    "\n",
    "In the Tableau dashboard, I want to enable filtering by the following Measurement Types:\n",
    "\n",
    "- `Gross Ticket Sales Amount`\n",
    "- `Net Ticket Sales Amount`\n",
    "- `Promotional Tickets Amount`\n",
    "- `Cancelled Tickets Amount`\n",
    "- `Ticket Returns Amount`\n",
    "\n",
    "To achieve this, I will pivot these measurements into a single column.\n",
    "\n",
    "First, I'll remove the `Retailer Location City` column, as I'm not going to pivot to that level of granularity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490236d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = merged_df.drop(columns='Retailer Location City')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70357339",
   "metadata": {},
   "source": [
    "Checking the resulting table before proceeding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e274ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf1262a",
   "metadata": {},
   "source": [
    "After removing the column, I will group the DataFrame by the remaining categorical columns and sum the numerical columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963504a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df = merged_df.groupby([\n",
    "    'Retailer Location County', \n",
    "    'Game Category', \n",
    "    'Calendar Year', \n",
    "    'Calendar Month'\n",
    "]).sum()\n",
    "\n",
    "# Resetting the index of the grouped DataFrame.\n",
    "grouped_df = grouped_df.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfaa71eb",
   "metadata": {},
   "source": [
    "Creating a pivot table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1477ec7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivoting the sales amount columns into a long format.\n",
    "long_format_df = pd.melt(\n",
    "    frame=grouped_df, \n",
    "    id_vars=[\n",
    "        'Retailer Location County',\n",
    "        'Game Category',\n",
    "        'Calendar Year',\n",
    "        'Calendar Month'\n",
    "    ], \n",
    "    value_vars=[\n",
    "        'Gross Ticket Sales Amount', \n",
    "        'Net Ticket Sales Amount', \n",
    "        'Promotional Tickets Amount', \n",
    "        'Cancelled Tickets Amount', \n",
    "        'Ticket Returns Amount'\n",
    "    ],\n",
    "    var_name='Sales Type', \n",
    "    value_name='Amount'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6f76e3",
   "metadata": {},
   "source": [
    "The result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ada7fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "long_format_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb36323",
   "metadata": {},
   "source": [
    "Saving the DataFrame to a CSV file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433da6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "long_format_df.to_csv('sales_pivot_table_.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
